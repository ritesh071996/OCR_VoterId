{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import tabula\n",
    "import camelot\n",
    "import pdfminer\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "try:\n",
    "    from cStringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO \n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st attempt\n",
      "1st attempt\n",
      "1st attempt\n",
      "1st attempt\n",
      "1st attempt\n",
      "\n",
      "Axis-Column headers missing\n",
      "[]\n",
      "0\n",
      "replace NaN\n",
      "0 False\n",
      "1 False\n",
      "2 False\n",
      "3 True\n",
      "*****************************************\n",
      "4 False\n",
      "5 False\n",
      "6 True\n",
      "*****************************************\n",
      "7 False\n",
      "8 False\n",
      "9 False\n",
      "10 False\n",
      "11 False\n",
      "12 False\n",
      "13 False\n",
      "14 False\n",
      "15 False\n",
      "16 False\n",
      "17 False\n",
      "18 True\n",
      "*****************************************\n",
      "19 False\n",
      "20 True\n",
      "*****************************************\n",
      "21 False\n",
      "22 False\n",
      "23 True\n",
      "*****************************************\n",
      "24 False\n",
      "25 False\n",
      "26 True\n",
      "*****************************************\n",
      "27 False\n",
      "28 False\n",
      "29 True\n",
      "*****************************************\n",
      "30 False\n",
      "31 False\n",
      "32 False\n",
      "33 True\n",
      "*****************************************\n",
      "34 False\n",
      "35 False\n",
      "36 False\n",
      "37 False\n",
      "38 False\n",
      "39 False\n",
      "40 False\n",
      "41 False\n",
      "42 False\n",
      "43 False\n",
      "44 False\n",
      "45 False\n",
      "46 False\n",
      "47 False\n",
      "48 False\n",
      "49 False\n",
      "50 False\n",
      "51 False\n",
      "52 False\n",
      "53 False\n",
      "54 False\n",
      "55 False\n",
      "56 False\n",
      "57 False\n",
      "58 False\n",
      "59 False\n",
      "60 False\n",
      "61 False\n",
      "62 False\n",
      "63 False\n",
      "64 False\n",
      "65 False\n",
      "66 False\n",
      "67 False\n",
      "68 False\n",
      "69 False\n",
      "70 False\n",
      "71 False\n",
      "72 False\n",
      "73 False\n",
      "74 False\n",
      "75 False\n",
      "76 False\n",
      "77 False\n",
      "78 False\n",
      "79 False\n",
      "80 False\n",
      "81 False\n",
      "82 False\n",
      "83 False\n",
      "84 False\n",
      "85 False\n",
      "86 False\n",
      "87 True\n",
      "*****************************************\n",
      "88 False\n",
      "89 False\n",
      "90 False\n",
      "91 False\n",
      "92 False\n",
      "93 False\n",
      "94 False\n",
      "95 False\n",
      "96 False\n",
      "97 False\n",
      "98 False\n",
      "99 False\n",
      "100 False\n",
      "101 False\n",
      "102 False\n",
      "103 False\n",
      "104 True\n",
      "*****************************************\n",
      "105 False\n",
      "106 False\n",
      "107 False\n",
      "108 False\n",
      "109 True\n",
      "*****************************************\n",
      "110 False\n",
      "111 False\n",
      "112 False\n",
      "113 False\n",
      "114 False\n",
      "115 False\n",
      "116 False\n",
      "117 False\n",
      "118 False\n",
      "119 False\n",
      "120 False\n",
      "121 False\n",
      "122 False\n",
      "123 False\n",
      "124 False\n",
      "125 False\n",
      "126 False\n",
      "127 False\n",
      "128 False\n",
      "129 False\n",
      "130 False\n",
      "131 False\n",
      "132 True\n",
      "*****************************************\n",
      "133 False\n",
      "134 False\n",
      "135 False\n",
      "136 True\n",
      "*****************************************\n",
      "137 False\n",
      "138 False\n",
      "139 False\n",
      "140 False\n",
      "141 False\n",
      "142 False\n",
      "143 False\n",
      "144 True\n",
      "*****************************************\n",
      "145 False\n",
      "146 False\n",
      "147 True\n",
      "*****************************************\n",
      "148 False\n",
      "149 False\n",
      "150 True\n",
      "*****************************************\n",
      "151 False\n",
      "152 False\n",
      "153 True\n",
      "*****************************************\n",
      "154 False\n",
      "155 False\n",
      "156 False\n",
      "157 False\n",
      "158 False\n",
      "159 False\n",
      "160 False\n",
      "161 True\n",
      "*****************************************\n",
      "162 False\n",
      "163 False\n",
      "164 False\n",
      "165 False\n",
      "166 False\n",
      "167 False\n",
      "168 False\n",
      "169 True\n",
      "*****************************************\n",
      "170 False\n",
      "171 False\n",
      "172 False\n",
      "173 False\n",
      "174 False\n",
      "175 False\n",
      "176 False\n",
      "177 False\n",
      "178 False\n",
      "179 True\n",
      "*****************************************\n",
      "180 False\n",
      "181 False\n",
      "182 True\n",
      "*****************************************\n",
      "183 False\n",
      "184 False\n",
      "185 False\n",
      "186 True\n",
      "*****************************************\n",
      "187 False\n",
      "188 False\n",
      "189 False\n",
      "190 True\n",
      "*****************************************\n",
      "191 False\n",
      "192 False\n",
      "193 False\n",
      "194 True\n",
      "*****************************************\n",
      "195 False\n",
      "196 False\n",
      "197 False\n",
      "198 False\n",
      "199 False\n",
      "200 False\n",
      "201 False\n",
      "202 True\n",
      "*****************************************\n",
      "203 False\n",
      "204 False\n",
      "205 False\n",
      "206 True\n",
      "*****************************************\n",
      "207 False\n",
      "208 False\n",
      "209 True\n",
      "*****************************************\n",
      "210 False\n",
      "211 False\n",
      "212 False\n",
      "213 False\n",
      "214 True\n",
      "*****************************************\n",
      "215 False\n",
      "216 False\n",
      "217 False\n",
      "218 False\n",
      "219 True\n",
      "*****************************************\n",
      "220 False\n",
      "221 False\n",
      "222 True\n",
      "*****************************************\n",
      "223 False\n",
      "224 False\n",
      "225 True\n",
      "*****************************************\n",
      "226 False\n",
      "227 False\n",
      "228 False\n",
      "229 False\n",
      "230 False\n",
      "231 False\n",
      "232 False\n",
      "233 True\n",
      "*****************************************\n",
      "234 False\n",
      "235 False\n",
      "236 False\n",
      "237 False\n",
      "238 False\n",
      "239 True\n",
      "*****************************************\n",
      "240 False\n",
      "241 False\n",
      "242 False\n",
      "243 True\n",
      "*****************************************\n",
      "244 False\n",
      "245 False\n",
      "246 False\n",
      "247 True\n",
      "*****************************************\n",
      "248 False\n",
      "249 False\n",
      "250 True\n",
      "*****************************************\n",
      "251 False\n",
      "252 False\n",
      "253 True\n",
      "*****************************************\n",
      "254 False\n",
      "255 False\n",
      "256 False\n",
      "257 True\n",
      "*****************************************\n",
      "258 False\n",
      "259 False\n",
      "260 False\n",
      "261 False\n",
      "262 False\n",
      "263 False\n",
      "264 False\n",
      "265 False\n",
      "266 False\n",
      "267 False\n",
      "268 False\n",
      "269 False\n",
      "270 True\n",
      "*****************************************\n",
      "271 False\n",
      "272 False\n",
      "273 False\n",
      "274 False\n",
      "275 False\n",
      "276 False\n",
      "277 True\n",
      "*****************************************\n",
      "278 False\n",
      "279 False\n",
      "280 False\n",
      "281 False\n",
      "282 False\n",
      "283 True\n",
      "*****************************************\n",
      "284 False\n",
      "285 False\n",
      "286 False\n",
      "287 False\n",
      "288 True\n",
      "*****************************************\n",
      "289 False\n",
      "290 False\n",
      "291 False\n",
      "292 True\n",
      "*****************************************\n",
      "293 False\n",
      "294 False\n",
      "295 True\n",
      "*****************************************\n",
      "296 False\n",
      "297 False\n"
     ]
    }
   ],
   "source": [
    "def isnan(value):\n",
    "    try:\n",
    "        return math.isnan(float(value))\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def icici_p3(f,num):\n",
    "    tmp = pd.DataFrame()\n",
    "    tables = camelot.read_pdf(f, pages = str(num), flavor = \"stream\", edge_tol = 25)\n",
    "    tmp = tables[0].df\n",
    "    try:\n",
    "        idx=[ c for c in tmp[tmp.apply(lambda row: row.astype(str).str.lower().str.contains('balance').any(), axis=1) ==True].index if c in tmp[tmp.apply(lambda row: row.astype(str).str.lower().str.contains('date').any(), axis=1) ==True].index ][0]\n",
    "        tmp.columns=tmp.iloc[idx] ; tmp=tmp.iloc[idx+1:,:] ; tmp.reset_index(drop=True,inplace=True)\n",
    "        print(\"1st attempt\")\n",
    "    except:\n",
    "        try:\n",
    "            idx=[ c for c in tmp[tmp.apply(lambda row: row.astype(str).str.lower().str.contains('balance').any(), axis=1) ==True].index if c in tmp[tmp.apply(lambda row: row.astype(str).str.lower().str.contains('particular').any(), axis=1) ==True].index ][0]\n",
    "            tmp.columns=tmp.iloc[idx] ; tmp=tmp.iloc[idx+1:,:] ; tmp.reset_index(drop=True,inplace=True)           \n",
    "        except:\n",
    "            print(\"\\nAxis-Column headers missing\"); pass\n",
    "    return tmp\n",
    "\n",
    "def icici_p3_process(df):\n",
    "#     try:\n",
    "#         idx=[ c for c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('balance').any(), axis=1) ==True].index if c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('date').any(), axis=1) ==True].index ][0]\n",
    "#         df.columns=df.iloc[idx] ; df=df.iloc[idx+1:,:] ; df.reset_index(drop=True,inplace=True)\n",
    "#         print(\"1st attempt\")\n",
    "#     except:\n",
    "#         try:\n",
    "#             idx=[ c for c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('balance').any(), axis=1) ==True].index if c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('particular').any(), axis=1) ==True].index ][0]\n",
    "#             df.columns=df.iloc[idx] ; df=df.iloc[idx+1:,:] ; df.reset_index(drop=True,inplace=True)           \n",
    "#         except:\n",
    "#             print(\"\\nAxis-Column headers missing\"); pass\n",
    "\n",
    "    drop_idx=[c for c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('balance').any(),axis=1)==True].index if c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('date').any(),axis=1)==True].index]\n",
    "    print(drop_idx)\n",
    "    print(len(drop_idx))\n",
    "    if len(drop_idx)>0:\n",
    "        df.drop(df.index[drop_idx],inplace=True)\n",
    "    \n",
    "    df.to_csv(\"before.csv\", index =False)\n",
    "    \n",
    "    try:\n",
    "        bal=[c for c in df.columns if \"BALANCE\" in str(c).upper() ][0]\n",
    "    except: print(\"\\nBalance columns missing\")\n",
    "\n",
    "    try:\n",
    "        dat=[c for c in df.columns if \"TRANSACTION DATE\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            dat=[c for c in df.columns if \"TXN DATE\" in str(c).upper() ][0]\n",
    "        except:\n",
    "            try:\n",
    "                dat=[c for c in df.columns if \"DATE\" in str(c).upper() ][0]\n",
    "            except:pass\n",
    "\n",
    "    try:\n",
    "        chq=[c for c in df.columns if \"CHQ\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            chq=[c for c in df.columns if \"CHEQUE\" in str(c).upper() ][0]\n",
    "        except:\n",
    "            try:\n",
    "                df[\"CHQ\"] = np.nan\n",
    "                chq=[c for c in df.columns if \"CHQ\" in str(c).upper() ][0]\n",
    "            except:pass\n",
    "\n",
    "    try:\n",
    "        narr=[c for c in df.columns if \"REMARKS\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            narr=[c for c in df.columns if \"PARTICULARS\" in str(c).upper() ][0]\n",
    "        except:\n",
    "            try:\n",
    "                narr=[c for c in df.columns if \"DESCRIPTION\" in str(c).upper() ][0]\n",
    "            except:\n",
    "                try:\n",
    "                    narr=[c for c in df.columns if \"DETAILS\" in str(c).upper() ][0]\n",
    "                except:\n",
    "                    try:\n",
    "                        narr=[c for c in df.columns if \"NARRATION\" in str(c).upper() ][0]\n",
    "                    except:pass\n",
    "    \n",
    "    try:    \n",
    "        df.replace(r'^\\s*$', np.nan, regex=True, inplace = True)\n",
    "        print(\"replace NaN\")\n",
    "    except:pass\n",
    "    \n",
    "    df.to_csv(\"before2.csv\", index =False)\n",
    "    \n",
    "    check_df_narr = df[narr]\n",
    "    check_df_narr = check_df_narr.isnull()\n",
    "    #check_df_narr.to_csv(\"check_df_narr.csv\", index =False)\n",
    "    \n",
    "    df_narr = { 'Check_Narration': check_df_narr} \n",
    "    result = pd.DataFrame(df_narr)\n",
    "\n",
    "    for j,i in enumerate(result[\"Check_Narration\"]):\n",
    "        print(j,i)\n",
    "        if i == True:\n",
    "            df.loc[j,narr] = df[narr][j-1]+df[narr][j+1]\n",
    "            print(\"*****************************************\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df = df[ df.isnull().sum(axis=1) < df.shape[1]-2].reset_index(drop=True)\n",
    "    \n",
    "    df.to_csv(\"before3.csv\", index =False)\n",
    "\n",
    "    try:\n",
    "        wdl=[c for c in df.columns if \"WITHDRAW\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            wdl=[c for c in df.columns if \"DEBIT\" in str(c).upper() ][0]\n",
    "        except: pass \n",
    "\n",
    "    try:\n",
    "        dep=[c for c in df.columns if \"DEPOSIT\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            dep=[c for c in df.columns if \"CREDIT\" in str(c).upper() ][0]\n",
    "        except: pass\n",
    "\n",
    "    df[dep]=df[dep].apply( lambda x: x.split(' ')[0] if type(x) == str else x )\n",
    "    df[wdl]=df[wdl].apply( lambda x: x.split(' ')[0] if type(x) == str else x )\n",
    "    df[wdl]=df[wdl].astype(str).apply(lambda x: str(x).replace(\"\\r\",\"\").replace(\",\",\"\").replace(\"Cr\",\"\").replace(\"Dr\",\"\")).astype(float) *-1\n",
    "    df[dep]=df[dep].astype(str).apply(lambda x: str(x).replace(\"\\r\",\"\").replace(\",\",\"\").replace(\"Cr\",\"\").replace(\"Dr\",\"\")).astype(float)\n",
    "    df[bal]=df[bal].astype(str).apply(lambda x:str(x).replace(\"\\r\",\"\").replace(\",\",\"\").replace(\"Cr\",\"\").replace(\"Dr\",\"\")).astype(float)\n",
    "\n",
    "\n",
    "    df = df[[dat,chq,narr,wdl,dep,bal]]\n",
    "    df.columns = [\"Xns Date\",\"Cheque No\",\"Narration\",\"Debits\",\"Credits\",\"Balance\"]\n",
    "\n",
    "    df[\"Xns Date\"] = df[\"Xns Date\"].str.replace(\"\\r\",\"\")\n",
    "    \n",
    "    \n",
    "    list1 = []\n",
    "    for j,i in enumerate(df[\"Xns Date\"]):\n",
    "            if isnan(i) == True:\n",
    "                list1.append(j)\n",
    "    df.drop(df.index[list1],inplace=True)\n",
    "\n",
    "    df[\"Xns Date\"]=pd.to_datetime(df[\"Xns Date\"], dayfirst=True)\n",
    "    df[\"Xns Date\"]=df[\"Xns Date\"].dt.date.astype(str)\n",
    "    \n",
    "    if df[\"Xns Date\"][0]>df[\"Xns Date\"][len(df)-1]:\n",
    "        df = df.iloc[::-1]\n",
    "        df.reset_index(inplace = True, drop =True)\n",
    "    \n",
    "    df.to_excel(\"check.xlsx\", index=False)\n",
    "    return df\n",
    "    \n",
    "f = r\"C:\\Users\\rites\\Desktop\\mudracircle\\bks_raw\\Parsing_testing\\ICICI\\ICICI\\pattern_3\\icici01.pdf\"\n",
    "pdf = PdfFileReader(open(f,'rb'))\n",
    "total_cnt = pdf.getNumPages()\n",
    "\n",
    "dt = pd.DataFrame();\n",
    "for i in range(1,total_cnt + 1):\n",
    "    tmp = icici_p3(f, i)\n",
    "    if tmp.shape[1] == 6:\n",
    "        tmp = pd.DataFrame(tmp).replace(r'^\\s*$', np.nan, regex=True)\n",
    "        dt = pd.concat([dt, tmp]).reset_index(drop=True)\n",
    "        \n",
    "# dt.to_csv(\"check.csv\", index=False)\n",
    "df = icici_p3_process(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TableList n=7>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = r\"C:\\Users\\rites\\Desktop\\mudracircle\\bks_raw\\Parsing_testing\\ICICI\\ICICI\\pattern_3\\icici01.pdf\"\n",
    "tables=camelot.read_pdf(f, flavor=\"stream\", pages=\"all\")\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diiferent structure on page: 6\n"
     ]
    }
   ],
   "source": [
    "if len(tables) != 0:\n",
    "    dt = pd.DataFrame(); tmp = pd.DataFrame();\n",
    "    for i in range (len(tables)):\n",
    "        tmp = tables[i].df\n",
    "        if (tmp.shape[1] < 3):\n",
    "            print(\"\\nDiiferent structure on page:\",i); pass\n",
    "        elif (tmp.shape[1] == 6):\n",
    "            tmp = pd.DataFrame(tmp).replace(r'^\\s*$', np.nan, regex = True)\n",
    "            dt = pd.concat([dt,tmp]).reset_index(drop=True)\n",
    "        else:\n",
    "            print(\"\\nAnother Structure on page:\",i); pass\n",
    "        \n",
    "dt.to_csv(\"check01.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st attempt\n"
     ]
    }
   ],
   "source": [
    "df = dt\n",
    "try:\n",
    "    idx=[ c for c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('balance').any(), axis=1) ==True].index if c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('date').any(), axis=1) ==True].index ][0]\n",
    "    df.columns=df.iloc[idx] ; df=df.iloc[idx+1:,:] ; df.reset_index(drop=True,inplace=True)\n",
    "    print(\"1st attempt\")\n",
    "except:\n",
    "    try:\n",
    "        idx=[ c for c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('balance').any(), axis=1) ==True].index if c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('particular').any(), axis=1) ==True].index ][0]\n",
    "        df.columns=df.iloc[idx] ; df=df.iloc[idx+1:,:] ; df.reset_index(drop=True,inplace=True)           \n",
    "    except:\n",
    "        print(\"\\nAxis-Column headers missing\"); pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'check.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-7d3750e82ae1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"check.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rites\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rites\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             )\n\u001b[0;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rites\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'check.csv'"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"check.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(df.nunique(dropna=False)[(df.nunique(dropna=False) == 1)].index, axis=1,inplace=True)\n",
    "\n",
    "try:\n",
    "    narr=[n for n in df.columns if \"PARTICULARS\" in str(n).upper()][0]\n",
    "except:\n",
    "    try:\n",
    "        narr=[n for n in df.columns if \"NARRATION\" in str(n).upper()][0]\n",
    "    except:\n",
    "        try:\n",
    "            narr=[n for n in df.columns if \"DESCRIPTION\" in str(n).upper()][0] \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "try:\n",
    "    bal=[c for c in df.columns if \"BALANCE\" in str(c).upper()][0]\n",
    "except:\n",
    "    print(\"\\nBalance Column Missing\")\n",
    "\n",
    "try:    \n",
    "    df.replace(r'^\\s*$', np.nan, regex=True, inplace = True)\n",
    "except:pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnan(value):\n",
    "    try:\n",
    "        return math.isnan(float(value))\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "try:\n",
    "    df.dropna(axis=0, how='all',inplace=True)\n",
    "except:pass\n",
    "\n",
    "try:\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "except:pass\n",
    "\n",
    "try:\n",
    "    for j,i in enumerate(df[narr]):\n",
    "        if isnan(i) == True:\n",
    "            df[narr][j] = df[narr][j-1]+df[narr][j+1]\n",
    "        else:\n",
    "            pass\n",
    "except:pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df.dropna(subset=['Tran Date'], inplace = True)\n",
    "except:\n",
    "    try:\n",
    "        df.dropna(subset=['Value Date'], inplace = True)\n",
    "    except:pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df = df[ df.isnull().sum(axis=1) < df.shape[1]-2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"parsed_completed.csv\",index=0)\n",
    "print(\"parsed_completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Parsed\n"
     ]
    }
   ],
   "source": [
    "def tab(f):\n",
    "    try:\n",
    "        tables=tabula.read_pdf(f,\n",
    "                                  lattice=True,\n",
    "                                  pages=\"all\",\n",
    "                                  silent=True,\n",
    "                                  multiple_tables=True,\n",
    "                                  pandas_options={'header':None})\n",
    "        df = pd.DataFrame()\n",
    "        df = pd.concat([c for c in tables]).drop_duplicates()\n",
    "    except: pass\n",
    "    return df\n",
    "\n",
    "def process(df):\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    try:\n",
    "        df = df.replace(r'NA', np.nan, regex=True)\n",
    "    except:pass\n",
    "    try:\n",
    "        idx=[c for c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('balance').any(),axis=1)==True].index if c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('date').any(),axis=1)==True].index][0]\n",
    "        df.columns=df.iloc[idx]; df=df.iloc[idx+1:,:]; df.reset_index(drop=True, inplace=True)\n",
    "    except:\n",
    "        try:\n",
    "            idx=[c for c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('balance').any(),axis=1)==True].index if c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('description').any(),axis=1)==True].index][0]\n",
    "            df.columns=df.iloc[idx]; df=df.iloc[idx+1:,:]; df.reset_index(drop=True, inplace=True)\n",
    "        except:\n",
    "            print(\"\\nSBI Column Headers Missing\"); pass\n",
    "        try:\n",
    "            df = df.drop([\"Init.\"], axis=1)\n",
    "        except:\n",
    "            try:\n",
    "                df = df.drop([\"Branch Name\"], axis=1)\n",
    "            except:pass\n",
    "\n",
    "    df = df[~df.index.isin(df[df.apply(lambda row:row.astype(str).str.lower().str.contains('opening balance|transaction total|closing balance').any(),axis=1)==True].index)]\n",
    "    #df = pd.DataFrame(df.T.drop_duplicates().T)\n",
    "    #df.drop(df.nunique(dropna=False)[(df.nunique(dropna=False) == 1)].index, axis=1, inplace=True)\n",
    "\n",
    "    df.to_csv(\"parsed.csv\",index=0)\n",
    "    return df\n",
    "\n",
    "f = r\"C:\\Users\\MudraCircle\\Desktop\\bks_raw\\Parsing_testing\\ICICI\\files\\icici011pdf\"\n",
    "#f = r'C:\\Users\\MudraCircle\\Desktop\\bks_raw\\Parsing_testing\\ICICI\\files\\icici_meena_sarees\\icici08.pdf'\n",
    "try:\n",
    "    df = tab(f) ; df = process(df)\n",
    "    print(\"Parsed\")\n",
    "except:\n",
    "    print(\"Not Parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed..\n"
     ]
    }
   ],
   "source": [
    "f = r'C:/Users/MudraCircle/Desktop/bks_raw/Parsing_testing/ICICI/files/icici07.pdf'\n",
    "tables=camelot.read_pdf(f, flavor=\"lattice\", pages=\"all\")\n",
    "if len(tables) !=0 :\n",
    "    df=pd.DataFrame(); tmp=pd.DataFrame();\n",
    "    for i in range(len(tables)):\n",
    "        tmp=tables[i].df\n",
    "        if (tmp.shape[1]==8)|(tmp.shape[1]==7)|(tmp.shape[1]==6)|(tmp.shape[1]==5):\n",
    "            df=pd.concat([df,tmp]).drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "        else:\n",
    "            print(\"\\n different structure on page: \",i)\n",
    "            \n",
    "df=df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df=df[ df.isnull().sum(axis=1) < df.shape[1]-2].reset_index(drop=True)       \n",
    "df.drop(df.nunique(dropna=False)[(df.nunique(dropna=False) == 1)].index, axis=1,inplace=True)\n",
    "try:\n",
    "    idx=[ c for c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('balance').any(), axis=1) ==True].index if c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('date').any(), axis=1) ==True].index ][0]\n",
    "    df.columns=df.iloc[idx] ; df=df.iloc[idx+1:,:] ; df.reset_index(drop=True,inplace=True)           \n",
    "except:\n",
    "        try:\n",
    "            idx=[ c for c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('balance').any(), axis=1) ==True].index if c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('particular').any(), axis=1) ==True].index ][0]\n",
    "            df.columns=df.iloc[idx] ; df=df.iloc[idx+1:,:] ; df.reset_index(drop=True,inplace=True)           \n",
    "        except:\n",
    "            try:\n",
    "                idx=[ c for c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('date').any(), axis=1) ==True].index if c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('narration').any(), axis=1) ==True].index ][0]\n",
    "                df.columns=df.iloc[idx] ; df=df.iloc[idx+1:,:] ; df.reset_index(    drop=True,inplace=True)           \n",
    "            except:       print(\"\\nProcess-Column headers missing\") \n",
    "                    \n",
    "df=df[~df.index.isin(df[df.apply(lambda row: row.astype(str).str.lower().str.contains('page:|account status|total|reason for return|inward clg|opening balance|statement of a/c').any(), axis=1) ==True].index)]\n",
    "df=pd.DataFrame(df.T.drop_duplicates().T)\n",
    "df.drop(df.nunique(dropna=False)[(df.nunique(dropna=False) == 1)].index, axis=1,inplace=True)\n",
    "     \n",
    "# try:\n",
    "#     dft=pd.DataFrame(df.iloc[:,0].dropna().unique().astype(int)).reset_index()\n",
    "#     if sum(dft.iloc[:,1])-sum(dft.iloc[:,0])==dft.shape[0] :\n",
    "#         df.drop(df.iloc[:,0].name, axis=1,inplace=True)\n",
    "# except:  pass\n",
    "df=df.drop_duplicates()\n",
    "df.to_csv(\"parsed.csv\", index=0)\n",
    "print(\"parsed..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'NAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a12469fd7297>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\rites\\Desktop\\mudracircle\\bks_raw\\Parsing_testing\\ICICI\\ICICI\\pattern_3\\icici01.pdf\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0micici_p1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m;\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0micici_p1_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;31m# try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a12469fd7297>\u001b[0m in \u001b[0;36micici_p1_process\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwdl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwdl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwdl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwdl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbal\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbal\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dr\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rites\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   5696\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5697\u001b[0m             \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5698\u001b[1;33m             \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5699\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rites\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"astype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rites\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, filter, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rites\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mastype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[0mvals1d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[1;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rites\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    895\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'NAME'"
     ]
    }
   ],
   "source": [
    "def abc(a):\n",
    "    if type(a) == str:\n",
    "        if len(a.split(' '))==2:\n",
    "            z=a.split(' ')[1]\n",
    "        else:\n",
    "            z=a.split(' ')[0]\n",
    "    else:\n",
    "        z=a\n",
    "    return z\n",
    "\n",
    "def isnan(value):\n",
    "    try:\n",
    "        return math.isnan(float(value))\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def icici_p1(f):\n",
    "    tables=tabula.read_pdf(f,\n",
    "                              lattice=True,\n",
    "                              pages=\"all\",\n",
    "                              silent=True,\n",
    "                              multiple_tables=True,\n",
    "                              pandas_options={'header':None})\n",
    "    df = pd.DataFrame()\n",
    "    df = pd.concat([c for c in tables]).drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def icici_p1_process(df):\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df = df[ df.isnull().sum(axis=1) < df.shape[1]-2].reset_index(drop=True)\n",
    "\n",
    "    try:\n",
    "        idx=[c for c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('balance').any(),axis=1)==True].index if c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('date').any(),axis=1)==True].index][0]\n",
    "        df.columns=df.iloc[idx]; df=df.iloc[idx+1:,:]; df.reset_index(drop=True, inplace=True)\n",
    "    except:\n",
    "        try:\n",
    "            idx=[c for c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('balance').any(),axis=1)==True].index if c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('transaction').any(),axis=1)==True].index][0]\n",
    "            df.columns=df.iloc[idx]; df=df.iloc[idx+1:,:]; df.reset_index(drop=True, inplace=True)\n",
    "        except:\n",
    "            try:\n",
    "                idx=[ c for c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('date').any(), axis=1) ==True].index if c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('remarks').any(), axis=1) ==True].index ][0]\n",
    "                df.columns=df.iloc[idx] ; df=df.iloc[idx+1:,:] ; df.reset_index(drop=True,inplace=True)\n",
    "            except:\n",
    "                print(\"\\nICICI Column Headers Missing\"); pass\n",
    "    try:\n",
    "        idx2 = [c for c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('page total').any(),axis=1)==True].index][0]    \n",
    "        df.drop(df.index[idx2:], inplace=True)\n",
    "    except:pass\n",
    "\n",
    "    try:\n",
    "        df = df.loc[:, df.columns.notnull()]\n",
    "    except:pass\n",
    "\n",
    "    try:\n",
    "        dat=[c for c in df.columns if \"TRANSACTION DATE\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            dat=[c for c in df.columns if \"TXN DATE\" in str(c).upper() ][0]\n",
    "        except:\n",
    "            try:\n",
    "                dat=[c for c in df.columns if \"DATE\" in str(c).upper() ][0]\n",
    "            except:pass\n",
    "\n",
    "    try:\n",
    "        chq=[c for c in df.columns if \"CHQ\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            chq=[c for c in df.columns if \"CHEQUE\" in str(c).upper() ][0]\n",
    "        except:pass\n",
    "\n",
    "    try:\n",
    "        narr=[c for c in df.columns if \"REMARKS\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            narr=[c for c in df.columns if \"PARTICULARS\" in str(c).upper() ][0]\n",
    "        except:\n",
    "            try:\n",
    "                narr=[c for c in df.columns if \"DESCRIPTION\" in str(c).upper() ][0]\n",
    "            except:\n",
    "                try:\n",
    "                    narr=[c for c in df.columns if \"DETAILS\" in str(c).upper() ][0]\n",
    "                except:pass\n",
    "\n",
    "    try:\n",
    "        bal=[c for c in df.columns if \"BALANCE\" in str(c).upper() ][0]\n",
    "        df[\"bal\"]=df[bal].apply( lambda x: abc(x) )\n",
    "    except: print(\"\\nBalance columns missing\") \n",
    "\n",
    "    try:\n",
    "        wdl=[c for c in df.columns if \"WITHDRAW\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            wdl=[c for c in df.columns if \"DEBIT\" in str(c).upper() ][0]\n",
    "        except: pass \n",
    "\n",
    "    try:\n",
    "        dep=[c for c in df.columns if \"DEPOSIT\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            dep=[c for c in df.columns if \"CREDIT\" in str(c).upper() ][0]\n",
    "        except: pass\n",
    "\n",
    "    df[[wdl, dep]] = df[[wdl, dep]].replace({\"NA\":np.nan, \"-\":np.nan,\"0\":np.nan})\n",
    "\n",
    "    try:\n",
    "        auto=[c for c in df.columns if \"AUTOSWEEP\" in str(c).upper() ][0]\n",
    "        for j,i in enumerate(df[auto]):\n",
    "            if isnan(i) == False:\n",
    "                df[wdl][j] = df[auto][j]\n",
    "            else:\n",
    "                pass\n",
    "    except: pass\n",
    "\n",
    "    try:\n",
    "        rev=[c for c in df.columns if \"REVERSE\" in str(c).upper() ][0]\n",
    "        for j,i in enumerate(df[rev]):\n",
    "            if isnan(i) == False:\n",
    "                df[dep][j] = df[rev][j]\n",
    "            else:\n",
    "                pass\n",
    "    except: pass\n",
    "    \n",
    "    df[dep]=df[dep].apply( lambda x: x.split(' ')[0] if type(x) == str else x )\n",
    "    df[wdl]=df[wdl].apply( lambda x: x.split(' ')[0] if type(x) == str else x )\n",
    "    df[wdl]=df[wdl].astype(str).apply(lambda x: str(x).replace(\"\\r\",\"\").replace(\",\",\"\").replace(\"Cr\",\"\").replace(\"Dr\",\"\")).astype(float) *-1\n",
    "    df[dep]=df[dep].astype(str).apply(lambda x: str(x).replace(\"\\r\",\"\").replace(\",\",\"\").replace(\"Cr\",\"\").replace(\"Dr\",\"\")).astype(float)\n",
    "    df[bal]=df[bal].astype(str).apply(lambda x:str(x).replace(\"\\r\",\"\").replace(\",\",\"\").replace(\"Cr\",\"\").replace(\"Dr\",\"\")).astype(float)\n",
    "\n",
    "    try:\n",
    "        for i in df.index:\n",
    "        #     # create new column\n",
    "        #     if \"Dr\" in df[\"new_column\"][i].astype(str):\n",
    "        #         df[bal][i] = df[bal][i]*-1\n",
    "            if df[\"bal\"][i] == \"Dr\":\n",
    "                df[bal][i] = df[bal][i]*-1\n",
    "            else:\n",
    "                pass\n",
    "    except:pass\n",
    "\n",
    "    try:\n",
    "        df.drop(['bal'], axis=1, inplace=True)\n",
    "    except:pass\n",
    "\n",
    "    try:\n",
    "        df.drop([auto], axis=1, inplace=True)\n",
    "    except:pass\n",
    "\n",
    "    try:\n",
    "        df.drop([rev], axis=1, inplace=True)\n",
    "    except:pass\n",
    "\n",
    "    df = df[[dat,chq,narr,wdl,dep,bal]]\n",
    "    df.columns = [\"Xns Date\",\"Cheque No\",\"Narration\",\"Debits\",\"Credits\",\"Balance\"]\n",
    "        \n",
    "    df.to_csv(\"check.csv\", index=0)\n",
    "\n",
    "f = r\"C:\\Users\\rites\\Desktop\\mudracircle\\bks_raw\\Parsing_testing\\ICICI\\ICICI\\pattern_3\\icici01.pdf\"\n",
    "df = icici_p1(f) ; df = icici_p1_process(df)\n",
    "\n",
    "# try:\n",
    "#     df = icici_p1(f) ; df = icici_p1_process(df)\n",
    "#     print(\"Parsed\")\n",
    "# except Exception as e:\n",
    "#     print(\"Error:\",e)\n",
    "#     print(\"Not Parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed\n"
     ]
    }
   ],
   "source": [
    "def abc(a):\n",
    "    if type(a) == str:\n",
    "        if len(a.split(' '))==2:\n",
    "            z=a.split(' ')[1]\n",
    "        else:\n",
    "            z=a.split(' ')[0]\n",
    "    else:\n",
    "        z=a\n",
    "    return z\n",
    "\n",
    "def isnan(value):\n",
    "    try:\n",
    "        return math.isnan(float(value))\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def icici_p1(f):\n",
    "    tables=tabula.read_pdf(f,\n",
    "                              lattice=True,\n",
    "                              pages=\"all\",\n",
    "                              silent=True,\n",
    "                              multiple_tables=True,\n",
    "                              pandas_options={'header':None})\n",
    "    df = pd.DataFrame()\n",
    "    df = pd.concat([c for c in tables]).drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def icici_p1_process(df):\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df = df[ df.isnull().sum(axis=1) < df.shape[1]-2].reset_index(drop=True)\n",
    "\n",
    "    try:\n",
    "        idx=[c for c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('balance').any(),axis=1)==True].index if c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('date').any(),axis=1)==True].index][0]\n",
    "        df.columns=df.iloc[idx]; df=df.iloc[idx+1:,:]; df.reset_index(drop=True, inplace=True)\n",
    "    except:\n",
    "        try:\n",
    "            idx=[c for c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('balance').any(),axis=1)==True].index if c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('transaction').any(),axis=1)==True].index][0]\n",
    "            df.columns=df.iloc[idx]; df=df.iloc[idx+1:,:]; df.reset_index(drop=True, inplace=True)\n",
    "        except:\n",
    "            try:\n",
    "                idx=[ c for c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('date').any(), axis=1) ==True].index if c in df[df.apply(lambda row: row.astype(str).str.lower().str.contains('remarks').any(), axis=1) ==True].index ][0]\n",
    "                df.columns=df.iloc[idx] ; df=df.iloc[idx+1:,:] ; df.reset_index(drop=True,inplace=True)\n",
    "            except:\n",
    "                print(\"\\nICICI Column Headers Missing\"); pass\n",
    "    try:\n",
    "        idx2 = [c for c in df[df.apply(lambda row:row.astype(str).str.lower().str.contains('page total').any(),axis=1)==True].index][0]    \n",
    "        df.drop(df.index[idx2:], inplace=True)\n",
    "    except:pass\n",
    "\n",
    "    try:\n",
    "        df = df.loc[:, df.columns.notnull()]\n",
    "    except:pass\n",
    "\n",
    "    try:\n",
    "        dat=[c for c in df.columns if \"TRANSACTION DATE\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            dat=[c for c in df.columns if \"TXN DATE\" in str(c).upper() ][0]\n",
    "        except:\n",
    "            try:\n",
    "                dat=[c for c in df.columns if \"DATE\" in str(c).upper() ][0]\n",
    "            except:pass\n",
    "\n",
    "    try:\n",
    "        chq=[c for c in df.columns if \"CHQ\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            chq=[c for c in df.columns if \"CHEQUE\" in str(c).upper() ][0]\n",
    "        except:pass\n",
    "\n",
    "    try:\n",
    "        narr=[c for c in df.columns if \"REMARKS\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            narr=[c for c in df.columns if \"PARTICULARS\" in str(c).upper() ][0]\n",
    "        except:\n",
    "            try:\n",
    "                narr=[c for c in df.columns if \"DESCRIPTION\" in str(c).upper() ][0]\n",
    "            except:\n",
    "                try:\n",
    "                    narr=[c for c in df.columns if \"DETAILS\" in str(c).upper() ][0]\n",
    "                except:pass\n",
    "\n",
    "    try:\n",
    "        bal=[c for c in df.columns if \"BALANCE\" in str(c).upper() ][0]\n",
    "        df[\"bal\"]=df[bal].apply( lambda x: abc(x) )\n",
    "    except: print(\"\\nBalance columns missing\") \n",
    "\n",
    "    try:\n",
    "        wdl=[c for c in df.columns if \"WITHDRAW\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            wdl=[c for c in df.columns if \"DEBIT\" in str(c).upper() ][0]\n",
    "        except: pass \n",
    "\n",
    "    try:\n",
    "        dep=[c for c in df.columns if \"DEPOSIT\" in str(c).upper() ][0]\n",
    "    except:\n",
    "        try:\n",
    "            dep=[c for c in df.columns if \"CREDIT\" in str(c).upper() ][0]\n",
    "        except: pass\n",
    "\n",
    "    df[[wdl, dep]] = df[[wdl, dep]].replace({\"NA\":np.nan, \"-\":np.nan,\"0\":np.nan})\n",
    "\n",
    "    try:\n",
    "        auto=[c for c in df.columns if \"AUTOSWEEP\" in str(c).upper() ][0]\n",
    "        for j,i in enumerate(df[auto]):\n",
    "            if isnan(i) == False:\n",
    "                df[wdl][j] = df[auto][j]\n",
    "            else:\n",
    "                pass\n",
    "    except: pass\n",
    "\n",
    "    try:\n",
    "        rev=[c for c in df.columns if \"REVERSE\" in str(c).upper() ][0]\n",
    "        for j,i in enumerate(df[rev]):\n",
    "            if isnan(i) == False:\n",
    "                df[dep][j] = df[rev][j]\n",
    "            else:\n",
    "                pass\n",
    "    except: pass\n",
    "    \n",
    "    df[dep]=df[dep].apply( lambda x: x.split(' ')[0] if type(x) == str else x )\n",
    "    df[wdl]=df[wdl].apply( lambda x: x.split(' ')[0] if type(x) == str else x )\n",
    "    df[wdl]=df[wdl].astype(str).apply(lambda x: str(x).replace(\"\\r\",\"\").replace(\",\",\"\").replace(\"Cr\",\"\").replace(\"Dr\",\"\")).astype(float) *-1\n",
    "    df[dep]=df[dep].astype(str).apply(lambda x: str(x).replace(\"\\r\",\"\").replace(\",\",\"\").replace(\"Cr\",\"\").replace(\"Dr\",\"\")).astype(float)\n",
    "    df[bal]=df[bal].astype(str).apply(lambda x:str(x).replace(\"\\r\",\"\").replace(\",\",\"\").replace(\"Cr\",\"\").replace(\"Dr\",\"\")).astype(float)\n",
    "\n",
    "    try:\n",
    "        for i in df.index:\n",
    "        #     # create new column\n",
    "        #     if \"Dr\" in df[\"new_column\"][i].astype(str):\n",
    "        #         df[bal][i] = df[bal][i]*-1\n",
    "            if df[\"bal\"][i] == \"Dr\":\n",
    "                df[bal][i] = df[bal][i]*-1\n",
    "            else:\n",
    "                pass\n",
    "    except:pass\n",
    "\n",
    "    try:\n",
    "        df.drop(['bal'], axis=1, inplace=True)\n",
    "    except:pass\n",
    "\n",
    "    try:\n",
    "        df.drop([auto], axis=1, inplace=True)\n",
    "    except:pass\n",
    "\n",
    "    try:\n",
    "        df.drop([rev], axis=1, inplace=True)\n",
    "    except:pass\n",
    "\n",
    "    df = df[[dat,chq,narr,wdl,dep,bal]]\n",
    "    df.columns = [\"Xns Date\",\"Cheque No\",\"Narration\",\"Debits\",\"Credits\",\"Balance\"]\n",
    "        \n",
    "    df.to_csv(\"check_csv.csv\", index=0)\n",
    "    #df.to_xml(\"check_xml.xml\")\n",
    "    df.to_excel(\"check_excel.xlsx\", index=False)\n",
    "\n",
    "f = r\"C:\\Users\\MudraCircle\\Desktop\\bks_raw\\Parsing_testing\\SBI\\files\\sbi14.pdf\"\n",
    "try:\n",
    "    df = icici_p1(f) ; df = icici_p1_process(df)\n",
    "    print(\"Parsed\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\",e)\n",
    "    print(\"Not Parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_xml(df, filename=\"None.xml\", mode='w'):\n",
    "    def row_to_xml(row):\n",
    "        xml = ['<item>']\n",
    "        for i, col_name in enumerate(row.index):\n",
    "            xml.append('  <field name=\"{0}\">{1}</field>'.format(col_name, row.iloc[i]))\n",
    "        xml.append('</item>')\n",
    "        return '\\n'.join(xml)\n",
    "    res = '\\n'.join(df.apply(row_to_xml, axis=1))\n",
    "\n",
    "    if filename is None:\n",
    "        return res\n",
    "    with open(filename, mode) as f:\n",
    "        f.write(res)\n",
    "\n",
    "pd.DataFrame.to_xml = to_xml\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
